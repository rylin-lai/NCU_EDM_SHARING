#!/usr/bin/env python3
"""
TLSH åˆ†æå·¥å…· - æ”¯æ´å…©ç¨®ä½¿ç”¨æ¡ˆä¾‹
TLSH Analyzer Tool - Supporting Two Use Cases

æ¡ˆä¾‹1: æ¯”è¼ƒå…©å€‹æ–‡å­—çš„ç›¸ä¼¼æ€§ (é©ç”¨æ–¼ä¼æ¥­è³‡æ–™å¤–æ´©èª¿æŸ¥)
Case 1: Compare similarity between two texts (for enterprise data leak investigation)

æ¡ˆä¾‹2: å°è³‡æ–™é›†é€²è¡Œ DBSCAN åˆ†ç¾¤åˆ†æ
Case 2: DBSCAN clustering analysis on datasets
"""

import argparse
import sys
import os
import json
import tlsh
from typing import List, Tuple, Dict

# æ·»åŠ  pylib åˆ°è·¯å¾‘ä»¥ä¾¿å°å…¥ / Add pylib to path for imports
sys.path.append('./pylib')
try:
    from tlsh_lib import tlsh_csvfile, runDBSCAN, tlist2cdata, sim_affinity
    from printCluster import outputClusters
except ImportError:
    print("éŒ¯èª¤ï¼šç„¡æ³•å°å…¥ TLSH å‡½å¼åº«ã€‚è«‹ç¢ºä¿ pylib ç›®éŒ„å­˜åœ¨ã€‚")
    print("Error: Could not import TLSH libraries. Make sure pylib directory exists.")
    sys.exit(1)


class TLSHAnalyzer:
    """TLSH åˆ†æå™¨é¡åˆ¥ / TLSH Analyzer Class"""
    
    def __init__(self):
        self.verbose = False
    
    def calculate_tlsh_hash(self, text: str) -> str:
        """
        è¨ˆç®—æ–‡å­—çš„ TLSH é›œæ¹Šå€¼
        Calculate TLSH hash for text content
        
        Args:
            text: è¼¸å…¥æ–‡å­— / Input text
        Returns:
            TLSH é›œæ¹Šå­—ä¸² / TLSH hash string
        """
        if len(text) < 50:  # TLSH éœ€è¦æœ€å°é•·åº¦ / TLSH requires minimum length
            raise ValueError("æ–‡å­—å¤ªçŸ­ï¼Œç„¡æ³•è¨ˆç®— TLSHï¼ˆæœ€å°‘éœ€è¦50å­—å…ƒï¼‰/ Text too short for TLSH calculation (minimum 50 characters)")
        
        tlsh_obj = tlsh.Tlsh()
        tlsh_obj.update(text.encode('utf-8'))
        tlsh_obj.final()
        return tlsh_obj.hexdigest()
    
    def compare_two_texts(self, text1: str, text2: str) -> Dict:
        """
        æ¡ˆä¾‹ 1: æ¯”è¼ƒå…©å€‹æ–‡å­—çš„ç›¸ä¼¼æ€§
        Case 1: Compare similarity between two texts
        
        é€™å€‹åŠŸèƒ½é©ç”¨æ–¼ä¼æ¥­è³‡æ–™å¤–æ´©èª¿æŸ¥ï¼Œå¯ä»¥å¿«é€Ÿåˆ¤æ–·å…©å€‹æª”æ¡ˆæ˜¯å¦åŒ…å«ç›¸åŒè³‡æ–™
        This function is suitable for enterprise data leak investigation,
        to quickly determine if two files contain the same data
        """
        if self.verbose:
            print("ğŸ” æ¡ˆä¾‹ 1: æ¯”è¼ƒå…©å€‹æ–‡å­—å­—ä¸²çš„ç›¸ä¼¼æ€§")
            print("ğŸ” Case 1: Comparing two text strings")
        
        try:
            # è¨ˆç®— TLSH é›œæ¹Š / Calculate TLSH hashes
            hash1 = self.calculate_tlsh_hash(text1)
            hash2 = self.calculate_tlsh_hash(text2)
            
            # è¨ˆç®—è·é›¢ / Calculate distance
            t1 = tlsh.Tlsh()
            t1.fromTlshStr(hash1)
            t2 = tlsh.Tlsh()
            t2.fromTlshStr(hash2)
            
            distance = t1.diff(t2)
            
            # åˆ†é¡ç›¸ä¼¼æ€§ / Classify similarity
            if distance == 0:
                similarity_class = "å®Œå…¨ç›¸åŒ / Identical"
                risk_level = "ç„¡é¢¨éšª / None"
            elif distance <= 50:
                similarity_class = "éå¸¸ç›¸ä¼¼ / Very Similar"
                risk_level = "é«˜é¢¨éšª / High"
            elif distance <= 100:
                similarity_class = "ç›¸ä¼¼ / Similar"
                risk_level = "ä¸­ç­‰é¢¨éšª / Medium"
            else:
                similarity_class = "ä¸åŒ / Different"
                risk_level = "ä½é¢¨éšª / Low"
            
            result = {
                "case": "two_text_comparison",
                "text1_length": len(text1),
                "text2_length": len(text2),
                "tlsh_hash1": hash1,
                "tlsh_hash2": hash2,
                "distance": distance,
                "similarity_class": similarity_class,
                "risk_level": risk_level,
                "interpretation": {
                    "zh": f"è·é›¢ {distance}: {similarity_class}ï¼Œé¢¨éšªç­‰ç´šï¼š{risk_level.split(' / ')[0]}",
                    "en": f"Distance {distance}: {similarity_class.split(' / ')[1]}, Risk level: {risk_level.split(' / ')[1]}"
                }
            }
            
            if self.verbose:
                print(f"  è·é›¢ / Distance: {distance}")
                print(f"  åˆ†é¡ / Classification: {similarity_class}")
                print(f"  é¢¨éšªç­‰ç´š / Risk Level: {risk_level}")
            
            return result
            
        except Exception as e:
            raise ValueError(f"æ¯”è¼ƒæ–‡å­—æ™‚ç™¼ç”ŸéŒ¯èª¤ / Error comparing texts: {str(e)}")
    
    def analyze_file_dataset(self, csv_file: str, eps: int = 30, min_samples: int = 2) -> Dict:
        """
        æ¡ˆä¾‹ 2: åˆ†ææª”æ¡ˆè³‡æ–™é›†çš„åˆ†ç¾¤
        Case 2: Analyze clustering of file dataset
        
        ä½¿ç”¨ DBSCAN æ¼”ç®—æ³•å° TLSH é›œæ¹Šé€²è¡Œåˆ†ç¾¤ï¼Œé©ç”¨æ–¼æƒ¡æ„è»Ÿé«”å®¶æ—åˆ†é¡æˆ–è³‡æ–™è­œç³»è¿½è¹¤
        Use DBSCAN algorithm to cluster TLSH hashes, suitable for malware family classification
        or data lineage tracking
        """
        if self.verbose:
            print(f"ğŸ” æ¡ˆä¾‹ 2: åˆ†æè³‡æ–™é›† {csv_file} çš„åˆ†ç¾¤")
            print(f"ğŸ” Case 2: Analyzing dataset clustering from {csv_file}")
        
        if not os.path.exists(csv_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° CSV æª”æ¡ˆ / CSV file not found: {csv_file}")
        
        # å¾ CSV è¼‰å…¥ TLSH è³‡æ–™ / Load TLSH data from CSV
        try:
            (tlist, labels) = tlsh_csvfile(csv_file)
            if tlist is None:
                raise ValueError("ç„¡æ³•å¾ CSV è¼‰å…¥ TLSH è³‡æ–™ / Failed to load TLSH data from CSV")
            
            if self.verbose:
                print(f"  å·²è¼‰å…¥ {len(tlist)} å€‹ TLSH é›œæ¹Š")
                print(f"  Loaded {len(tlist)} TLSH hashes")
            
            # åŸ·è¡Œ DBSCAN åˆ†ç¾¤ / Run DBSCAN clustering
            dbscan_result = runDBSCAN(tlist, eps=eps, min_samples=min_samples)
            cluster_labels = dbscan_result.labels_
            
            # åˆ†æçµæœ / Analyze results
            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
            n_noise = list(cluster_labels).count(-1)
            n_clustered = len(cluster_labels) - n_noise
            
            # å–å¾—å®¶æ—è³‡è¨Šï¼ˆå¦‚æœå¯ç”¨ï¼‰/ Get family information if available
            family_labels = labels[0] if labels and len(labels) > 0 else None
            unique_families = len(set(family_labels)) if family_labels else 0
            
            result = {
                "case": "dataset_clustering",
                "csv_file": csv_file,
                "total_samples": len(tlist),
                "clustering_params": {
                    "eps": eps,
                    "min_samples": min_samples
                },
                "results": {
                    "n_clusters": n_clusters,
                    "n_noise": n_noise,
                    "n_clustered": n_clustered,
                    "clustering_efficiency": round((n_clustered / len(tlist)) * 100, 2)
                },
                "family_info": {
                    "unique_families": unique_families,
                    "has_family_labels": family_labels is not None
                },
                "summary": {
                    "zh": f"æ‰¾åˆ° {n_clusters} å€‹ç¾¤é›†ï¼Œ{n_noise} å€‹é›œè¨Šé»ï¼Œåˆ†ç¾¤æ•ˆç‡ {round((n_clustered / len(tlist)) * 100, 2)}%",
                    "en": f"Found {n_clusters} clusters, {n_noise} noise points, clustering efficiency {round((n_clustered / len(tlist)) * 100, 2)}%"
                }
            }
            
            if self.verbose:
                print(f"  æ‰¾åˆ°ç¾¤é›† / Clusters found: {n_clusters}")
                print(f"  é›œè¨Šé» / Noise points: {n_noise}")
                print(f"  åˆ†ç¾¤æ•ˆç‡ / Clustering efficiency: {result['results']['clustering_efficiency']}%")
            
            return result
            
        except Exception as e:
            raise ValueError(f"åˆ†æè³‡æ–™é›†æ™‚ç™¼ç”ŸéŒ¯èª¤ / Error analyzing dataset: {str(e)}")
    
    def run_case_1_example(self) -> Dict:
        """åŸ·è¡Œæ¡ˆä¾‹ 1 çš„é è¨­ç¯„ä¾‹ / Run default example for Case 1"""
        # ä¼æ¥­è³‡æ–™å¤–æ´©æƒ…å¢ƒç¯„ä¾‹ / Enterprise data leak scenario example
        original_data = '''
        {"customer_id": "CUST_12345", "name": "å¼µå°æ˜", "email": "ming.zhang@email.com", 
         "phone": "+886-2-1234-5678", "address": "å°åŒ—å¸‚ä¿¡ç¾©å€ä¿¡ç¾©è·¯äº”æ®µ7è™Ÿ", 
         "order_history": [{"order_id": "ORD_001", "product": "ä¼æ¥­è»Ÿé«”æˆæ¬Š", "amount": 8999.99}],
         "account_status": "active", "created_date": "2023-12-01"}
        ''' * 3
        
        modified_data = '''
        {"customer_id": "CUST_12345", "name": "å¼µå°æ˜", "email": "ming.zhang@email.com", 
         "phone": "+886-2-1234-5678", "address": "å°åŒ—å¸‚ä¿¡ç¾©å€ä¿¡ç¾©è·¯äº”æ®µ7è™Ÿ", 
         "order_history": [{"order_id": "ORD_001", "product": "ä¼æ¥­è»Ÿé«”æˆæ¬Š", "amount": 8999.99}],
         "account_status": "active", "created_date": "2023-12-01", "last_access": "2024-03-15"}
        ''' * 3
        
        return self.compare_two_texts(original_data, modified_data)
    
    def run_case_2_example(self) -> Dict:
        """åŸ·è¡Œæ¡ˆä¾‹ 2 çš„é è¨­ç¯„ä¾‹ / Run default example for Case 2"""
        csv_file = "./data/mb_1K.csv"
        if os.path.exists(csv_file):
            return self.analyze_file_dataset(csv_file)
        else:
            # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå»ºç«‹æœ€å°ç¯„ä¾‹ / Create minimal example if file doesn't exist
            return {
                "case": "dataset_clustering",
                "csv_file": csv_file,
                "error": {
                    "zh": "æ‰¾ä¸åˆ°ç¯„ä¾‹è³‡æ–™é›†ã€‚è«‹æä¾›åŒ…å« TLSH è³‡æ–™çš„æœ‰æ•ˆ CSV æª”æ¡ˆã€‚",
                    "en": "Sample dataset not found. Please provide a valid CSV file with TLSH data."
                }
            }


def main():
    parser = argparse.ArgumentParser(
        description="TLSH åˆ†æå™¨ - æ”¯æ´å…©ç¨® TLSH ä½¿ç”¨æ¡ˆä¾‹çš„åˆ†æå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹ / Examples:

  æ¡ˆä¾‹ 1 - æ¯”è¼ƒå…©å€‹æ–‡å­— / Case 1 - Compare two texts:
    python tlsh_analyzer.py case1 --text1 "æ‚¨çš„ç¬¬ä¸€å€‹æ–‡å­—..." --text2 "æ‚¨çš„ç¬¬äºŒå€‹æ–‡å­—..."
    python tlsh_analyzer.py case1 --example  # ä½¿ç”¨å…§å»ºç¯„ä¾‹ / Use built-in example

  æ¡ˆä¾‹ 2 - åˆ†æè³‡æ–™é›† / Case 2 - Analyze dataset:
    python tlsh_analyzer.py case2 --csv data/mb_1K.csv
    python tlsh_analyzer.py case2 --example  # ä½¿ç”¨å…§å»ºç¯„ä¾‹ / Use built-in example
    python tlsh_analyzer.py case2 --csv data/mb_1K.csv --eps 20 --min_samples 3

  ä¸€èˆ¬é¸é … / General options:
    --verbose    é¡¯ç¤ºè©³ç´°è¼¸å‡º / Show detailed output
    --output     å°‡çµæœä¿å­˜ç‚º JSON æª”æ¡ˆ / Save results to JSON file
        """)
    
    parser.add_argument('case', choices=['case1', 'case2'], 
                       help='é¸æ“‡åˆ†ææ¡ˆä¾‹ / Choose analysis case')
    parser.add_argument('--verbose', '-v', action='store_true', 
                       help='å•Ÿç”¨è©³ç´°è¼¸å‡º / Enable verbose output')
    parser.add_argument('--output', '-o', type=str, 
                       help='çµæœè¼¸å‡ºæª”æ¡ˆ (JSON) / Output file for results (JSON)')
    parser.add_argument('--example', action='store_true', 
                       help='åŸ·è¡Œå…§å»ºç¯„ä¾‹ / Run built-in example')
    
    # æ¡ˆä¾‹ 1 å°ˆç”¨åƒæ•¸ / Case 1 specific arguments
    parser.add_argument('--text1', type=str, help='æ¯”è¼ƒçš„ç¬¬ä¸€å€‹æ–‡å­— / First text for comparison')
    parser.add_argument('--text2', type=str, help='æ¯”è¼ƒçš„ç¬¬äºŒå€‹æ–‡å­— / Second text for comparison')
    
    # æ¡ˆä¾‹ 2 å°ˆç”¨åƒæ•¸ / Case 2 specific arguments
    parser.add_argument('--csv', type=str, help='åŒ…å« TLSH è³‡æ–™çš„ CSV æª”æ¡ˆ / CSV file with TLSH data')
    parser.add_argument('--eps', type=int, default=30, help='DBSCAN eps åƒæ•¸ (é è¨­: 30) / DBSCAN eps parameter (default: 30)')
    parser.add_argument('--min_samples', type=int, default=2, help='DBSCAN min_samples åƒæ•¸ (é è¨­: 2) / DBSCAN min_samples parameter (default: 2)')
    
    args = parser.parse_args()
    
    analyzer = TLSHAnalyzer()
    analyzer.verbose = args.verbose
    
    try:
        if args.case == 'case1':
            if args.example:
                result = analyzer.run_case_1_example()
            elif args.text1 and args.text2:
                result = analyzer.compare_two_texts(args.text1, args.text2)
            else:
                print("éŒ¯èª¤ï¼šæ¡ˆä¾‹ 1 éœ€è¦ --text1 å’Œ --text2 åƒæ•¸ï¼Œæˆ–ä½¿ç”¨ --example")
                print("Error: Case 1 requires --text1 and --text2 arguments, or use --example")
                return 1
                
        elif args.case == 'case2':
            if args.example:
                result = analyzer.run_case_2_example()
            elif args.csv:
                result = analyzer.analyze_file_dataset(args.csv, args.eps, args.min_samples)
            else:
                print("éŒ¯èª¤ï¼šæ¡ˆä¾‹ 2 éœ€è¦ --csv åƒæ•¸ï¼Œæˆ–ä½¿ç”¨ --example")
                print("Error: Case 2 requires --csv argument, or use --example")
                return 1
        
        # è¼¸å‡ºçµæœ / Output results
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            if args.verbose:
                print(f"çµæœå·²å„²å­˜è‡³ {args.output}")
                print(f"Results saved to {args.output}")
        
        if not args.verbose or not args.output:
            print(json.dumps(result, indent=2, ensure_ascii=False))
        
        return 0
        
    except Exception as e:
        print(f"éŒ¯èª¤ / Error: {str(e)}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    sys.exit(main())
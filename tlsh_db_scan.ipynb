{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¡ The Solution: TLSH + Python Data Lineage / è§£æ±ºæ–¹æ¡ˆ\n",
    "**Step 1**: Collect JSON/CSV/log files from different departments/servers  \n",
    "**æ­¥é©Ÿ 1**: å¾ä¸åŒéƒ¨é–€/ä¼ºæœå™¨æ”¶é›† JSON/CSV/log æª”æ¡ˆ\n",
    "\n",
    "**Step 2**: Calculate TLSH hashes for content fingerprinting  \n",
    "**æ­¥é©Ÿ 2**: è¨ˆç®— TLSH é›œæ¹Šé€²è¡Œå…§å®¹æŒ‡ç´‹è­˜åˆ¥\n",
    "- Detects minor modifications (timestamps, formatting) / æª¢æ¸¬å¾®å°ä¿®æ”¹ï¼ˆæ™‚é–“æˆ³ã€æ ¼å¼ï¼‰\n",
    "- Fast similarity checks: \"Are files A and B the same data?\" / å¿«é€Ÿç›¸ä¼¼æ€§æª¢æŸ¥\n",
    "\n",
    "**Step 3**: Cluster files into \"Data Families\" using DBSCAN  \n",
    "**æ­¥é©Ÿ 3**: ä½¿ç”¨ DBSCAN å°‡æª”æ¡ˆåˆ†ç¾¤ç‚ºã€Œè³‡æ–™å®¶æ—ã€\n",
    "- Distance matrix from TLSH comparisons / å¾ TLSH æ¯”è¼ƒå¾—å‡ºè·é›¢çŸ©é™£\n",
    "- Visualize as lineage graph with networkx/matplotlib / ç”¨ networkx/matplotlib è¦–è¦ºåŒ–ç‚ºè­œç³»åœ–\n",
    "\n",
    "**Step 4**: Business Value / å•†æ¥­åƒ¹å€¼\n",
    "- ğŸ¯ Find incorrectly copied or leaked data / æ‰¾å‡ºéŒ¯èª¤è¤‡è£½æˆ–æ´©æ¼çš„è³‡æ–™\n",
    "- ğŸ”— Trace who modified/copied to which server / è¿½è¹¤èª°ä¿®æ”¹/è¤‡è£½åˆ°å“ªå€‹ä¼ºæœå™¨\n",
    "- ğŸ›¡ï¸ Improve internal data governance / æ”¹å–„å…§éƒ¨è³‡æ–™æ²»ç†\n",
    "\n",
    "**Why TLSH instead of traditional hashing? / ç‚ºä»€éº¼é¸æ“‡ TLSH è€Œéå‚³çµ±é›œæ¹Šï¼Ÿ**\n",
    "- Traditional MD5/SHA: Single bit change = completely different hash  \n",
    "  å‚³çµ± MD5/SHAï¼šå–®ä¸€ä½å…ƒè®Šæ›´ = å®Œå…¨ä¸åŒçš„é›œæ¹Šå€¼\n",
    "- TLSH: Similar content = similar hash values (allows fuzzy matching)  \n",
    "  TLSHï¼šç›¸ä¼¼å…§å®¹ = ç›¸ä¼¼é›œæ¹Šå€¼ï¼ˆå…è¨±æ¨¡ç³Šæ¯”å°ï¼‰\n",
    "- Perfect for detecting data variants with minor changes (formatting, timestamps)  \n",
    "  éå¸¸é©åˆæª¢æ¸¬æœ‰å¾®å°è®Šæ›´çš„è³‡æ–™è®Šç¨®ï¼ˆæ ¼å¼ã€æ™‚é–“æˆ³ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup / æ­¥é©Ÿ 1ï¼šç’°å¢ƒè¨­ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries / å°å…¥å¿…è¦çš„å‡½å¼åº«\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the tlshCluster library path / æ·»åŠ  tlshCluster å‡½å¼åº«è·¯å¾‘\n",
    "sys.path.append('./tlshCluster')\n",
    "sys.path.append('./tlshCluster/pylib')\n",
    "\n",
    "try:\n",
    "    import tlsh\n",
    "    from pylib.tlsh_lib import *\n",
    "    from pylib.hac_lib import *\n",
    "    print(\"âœ… All libraries loaded successfully\")\n",
    "    print(\"âœ… æ‰€æœ‰å‡½å¼åº«è¼‰å…¥æˆåŠŸ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"âŒ å°å…¥éŒ¯èª¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Dataset / æ­¥é©Ÿ 2ï¼šè¼‰å…¥å’Œæ¢ç´¢è³‡æ–™é›†\n",
    "\n",
    "### ğŸ“Š Dataset Input Format / è³‡æ–™é›†è¼¸å…¥æ ¼å¼\n",
    "\n",
    "**Input: CSV file with TLSH hashes / è¼¸å…¥ï¼šåŒ…å« TLSH é›œæ¹Šçš„ CSV æª”æ¡ˆ**\n",
    "```\n",
    "TLSH_Hash,Family_Label,File_Info\n",
    "T1A2B3C4D5E6F7G8...,Quakbot,sample001.exe\n",
    "T1B3C4D5E6F7G8H9...,Heodo,sample002.bin\n",
    "T1C4D5E6F7G8H9I0...,AgentTesla,sample003.dll\n",
    "```\n",
    "\n",
    "**TLSH Feature Structure / TLSH ç‰¹å¾µçµæ§‹:**\n",
    "- **Length**: 70 characters (å›ºå®šé•·åº¦) / Fixed 70 characters\n",
    "- **Components**: Header(6) + Checksum(2) + Body(62) / çµ„æˆï¼šæ¨™é ­(6) + æª¢æŸ¥ç¢¼(2) + ä¸»é«”(62)\n",
    "- **Encoding**: Hexadecimal representation of file characteristics / ç·¨ç¢¼ï¼šæª”æ¡ˆç‰¹å¾µçš„åå…­é€²ä½è¡¨ç¤º\n",
    "\n",
    "**Feature Extraction Process / ç‰¹å¾µæå–éç¨‹:**\n",
    "1. **File â†’ Bytes**: Raw binary content / åŸå§‹äºŒé€²ä½å…§å®¹\n",
    "2. **Bytes â†’ Sliding Window**: 5-byte windows with step size / 5ä½å…ƒçµ„æ»‘å‹•è¦–çª—\n",
    "3. **Windows â†’ Buckets**: 256 buckets for byte patterns / 256å€‹æ¡¶ç”¨æ–¼ä½å…ƒçµ„æ¨¡å¼\n",
    "4. **Buckets â†’ TLSH**: Quartile statistics + checksum / å››åˆ†ä½æ•¸çµ±è¨ˆ + æª¢æŸ¥ç¢¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 1K malware dataset using repository's function\n",
    "csv_filename = \"./data/mb_1K.csv\"\n",
    "\n",
    "print(\"ğŸ“‚ Loading real malware dataset / è¼‰å…¥çœŸå¯¦æƒ¡æ„è»Ÿé«”è³‡æ–™é›†\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "(tlist, labels) = tlsh_csvfile(csv_filename)\n",
    "\n",
    "if tlist is None:\n",
    "    print(\"âŒ Failed to load dataset\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"âœ… Loaded {len(tlist)} TLSH hashes\")\n",
    "print(f\"âœ… è¼‰å…¥äº† {len(tlist)} å€‹ TLSH é›œæ¹Š\")\n",
    "\n",
    "# Extract family labels\n",
    "family_labels = labels[0]  # labels[0] contains the signature/family names\n",
    "print(f\"ğŸ“Š Dataset contains {len(set(family_labels))} unique malware families\")\n",
    "print(f\"ğŸ“Š è³‡æ–™é›†åŒ…å« {len(set(family_labels))} å€‹å”¯ä¸€çš„æƒ¡æ„è»Ÿé«”å®¶æ—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze family distribution / åˆ†æå®¶æ—åˆ†å¸ƒ\n",
    "from collections import Counter\n",
    "\n",
    "family_counts = Counter(family_labels)\n",
    "print(\"\\nğŸ“Š Top 10 Malware Families / å‰10å¤§æƒ¡æ„è»Ÿé«”å®¶æ—:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for family, count in family_counts.most_common(10):\n",
    "    percentage = (count / len(family_labels)) * 100\n",
    "    print(f\"  {family:15}: {count:3d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Total families: {len(family_counts)}\")\n",
    "print(f\"ğŸ“ˆ ç¸½å®¶æ—æ•¸: {len(family_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: TLSH Distance Analysis / æ­¥é©Ÿ 3ï¼šTLSH è·é›¢åˆ†æ\n",
    "\n",
    "### ğŸ” Understanding TLSH as Features / ç†è§£ TLSH ä½œç‚ºç‰¹å¾µ\n",
    "\n",
    "**TLSH Technical Architecture / TLSH æŠ€è¡“æ¶æ§‹:**\n",
    "\n",
    "**1. File Processing Pipeline / æª”æ¡ˆè™•ç†ç®¡ç·š:**\n",
    "```\n",
    "Binary File â†’ Sliding Window (5-byte) â†’ Bucket Calculation (256 buckets) â†’ Statistics â†’ TLSH Hash\n",
    "äºŒé€²ä½æª”æ¡ˆ â†’ æ»‘å‹•è¦–çª—(5ä½å…ƒçµ„) â†’ æ¡¶è¨ˆç®—(256å€‹æ¡¶) â†’ çµ±è¨ˆè³‡æ–™ â†’ TLSHé›œæ¹Š\n",
    "```\n",
    "\n",
    "**2. TLSH Hash Structure (70 characters) / TLSH é›œæ¹Šçµæ§‹ï¼ˆ70å­—å…ƒï¼‰:**\n",
    "- **Header (6 chars)**: File length encoding / æª”æ¡ˆé•·åº¦ç·¨ç¢¼\n",
    "- **Checksum (2 chars)**: Content integrity check / å…§å®¹å®Œæ•´æ€§æª¢æŸ¥  \n",
    "- **Body (62 chars)**: Quartile statistics of bucket distributions / æ¡¶åˆ†å¸ƒçš„å››åˆ†ä½æ•¸çµ±è¨ˆ\n",
    "\n",
    "**3. Distance Calculation Algorithm / è·é›¢è¨ˆç®—æ¼”ç®—æ³•:**\n",
    "```python\n",
    "def tlsh_distance(hash1, hash2):\n",
    "    length_diff = abs(length1 - length2) * length_weight\n",
    "    checksum_diff = hamming_distance(checksum1, checksum2) * checksum_weight  \n",
    "    body_diff = quartile_diff(body1, body2) * body_weight\n",
    "    return length_diff + checksum_diff + body_diff\n",
    "```\n",
    "\n",
    "**4. Distance Interpretation / è·é›¢è§£é‡‹:**\n",
    "- **0-50**: Very similar (same data with minor formatting changes) / éå¸¸ç›¸ä¼¼ï¼ˆç›¸åŒè³‡æ–™æœ‰å¾®å°æ ¼å¼è®Šæ›´ï¼‰\n",
    "- **51-100**: Similar content structure / ç›¸ä¼¼å…§å®¹çµæ§‹\n",
    "- **101-200**: Moderately different / ä¸­ç­‰å·®ç•°  \n",
    "- **200+**: Very different content / å…§å®¹éå¸¸ä¸åŒ\n",
    "\n",
    "**5. Why TLSH Works for Data Lineage / ç‚ºä»€éº¼ TLSH é©ç”¨æ–¼è³‡æ–™è­œç³»:**\n",
    "- **Resilient to formatting**: CSV â†” JSON conversion doesn't drastically change hash / å°æ ¼å¼è®Šæ›´æœ‰æŠµæŠ—åŠ›\n",
    "- **Timestamp tolerant**: Adding/removing timestamps results in small distance changes / å°æ™‚é–“æˆ³å®¹å¿\n",
    "- **Content-aware**: Captures actual data patterns, not just byte sequences / å…§å®¹æ„ŸçŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample distance analysis / æ¨£æœ¬è·é›¢åˆ†æ\ndef calculate_sample_distances(tlist, family_labels, num_samples=5):\n    \"\"\"Calculate distances between random samples / è¨ˆç®—éš¨æ©Ÿæ¨£æœ¬é–“çš„è·é›¢\"\"\"\n    \n    print(\"ğŸ” TLSH Distance Analysis / TLSH è·é›¢åˆ†æ\")\n    print(\"=\" * 50)\n    \n    # Analyze different types of pairs using TLSH diff method\n    print(\"\\nğŸ“ Distance Examples / è·é›¢ç¯„ä¾‹:\")\n    \n    # Find some examples\n    if len(tlist) >= 2:\n        # Calculate distances using TLSH diff method\n        try:\n            # Use tlsh.diff() function directly with hash strings\n            dist1 = tlsh.diff(tlist[0], tlist[1])\n            family1, family2 = family_labels[0], family_labels[1]\n            print(f\"  Sample 0 vs Sample 1: {dist1} ({family1} vs {family2})\")\n            \n            if len(tlist) >= 12:\n                dist2 = tlsh.diff(tlist[0], tlist[10])\n                family3 = family_labels[10]\n                print(f\"  Sample 0 vs Sample 10: {dist2} ({family1} vs {family3})\")\n                \n                dist3 = tlsh.diff(tlist[1], tlist[11])\n                family4 = family_labels[11]\n                print(f\"  Sample 1 vs Sample 11: {dist3} ({family2} vs {family4})\")\n        except Exception as e:\n            print(f\"âš ï¸ Error calculating TLSH distances: {e}\")\n            print(\"Using simple string comparison for demonstration\")\n            dist1 = len(set(tlist[0]) - set(tlist[1])) + len(set(tlist[1]) - set(tlist[0]))\n            print(f\"  Sample 0 vs Sample 1: {dist1} (string diff)\")\n    \n    # Check for same family examples\n    quakbot_indices = [i for i, family in enumerate(family_labels) if family.lower() == 'quakbot']\n    if len(quakbot_indices) >= 2:\n        try:\n            dist = tlsh.diff(tlist[quakbot_indices[0]], tlist[quakbot_indices[1]])\n            print(f\"  Same family (Quakbot vs Quakbot): {dist}\")\n            print(f\"  åŒå®¶æ— (Quakbot vs Quakbot): {dist}\")\n        except Exception as e:\n            print(f\"  Same family calculation failed: {e}\")\n    \n    # Check for different families\n    agentTesla_indices = [i for i, family in enumerate(family_labels) if family.lower() == 'agenttesla']\n    if len(quakbot_indices) >= 1 and len(agentTesla_indices) >= 1:\n        try:\n            dist = tlsh.diff(tlist[quakbot_indices[0]], tlist[agentTesla_indices[0]])\n            print(f\"  Different families (Quakbot vs AgentTesla): {dist}\")\n            print(f\"  ä¸åŒå®¶æ— (Quakbot vs AgentTesla): {dist}\")\n        except Exception as e:\n            print(f\"  Different families calculation failed: {e}\")\n    \n    print(f\"\\nğŸ’¡ Distance Interpretation / è·é›¢è§£é‡‹:\")\n    print(f\"   0-50: Very similar (likely same data) / éå¸¸ç›¸ä¼¼ï¼ˆå¯èƒ½æ˜¯ç›¸åŒè³‡æ–™ï¼‰\")\n    print(f\"   51-100: Similar content / ç›¸ä¼¼å…§å®¹\")\n    print(f\"   101-200: Moderately different / ä¸­ç­‰å·®ç•°\")\n    print(f\"   200+: Very different / éå¸¸ä¸åŒ\")\n\ncalculate_sample_distances(tlist, family_labels)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run DBSCAN Clustering / æ­¥é©Ÿ 4ï¼šåŸ·è¡Œ DBSCAN åˆ†ç¾¤\n",
    "\n",
    "### ğŸ¯ DBSCAN Input/Output Specification / DBSCAN è¼¸å…¥/è¼¸å‡ºè¦æ ¼\n",
    "\n",
    "**Input to DBSCAN / DBSCAN è¼¸å…¥:**\n",
    "- **Feature Matrix**: 999 samples Ã— TLSH distance matrix / ç‰¹å¾µçŸ©é™£ï¼š999å€‹æ¨£æœ¬ Ã— TLSH è·é›¢çŸ©é™£\n",
    "- **Distance Function**: Custom TLSH.diff() / è·é›¢å‡½æ•¸ï¼šè‡ªå®šç¾© TLSH.diff()\n",
    "- **Parameters**: eps (distance threshold), min_samples / åƒæ•¸ï¼šepsï¼ˆè·é›¢é–¾å€¼ï¼‰ã€min_samples\n",
    "\n",
    "**DBSCAN Processing Pipeline / DBSCAN è™•ç†ç®¡ç·š:**\n",
    "```python\n",
    "# Data flow / è³‡æ–™æµç¨‹\n",
    "Raw_TLSH_Hashes â†’ Distance_Matrix â†’ DBSCAN_Algorithm â†’ Cluster_Labels\n",
    "     (999,70)         (999,999)           (eps,min)          (999,)\n",
    "```\n",
    "\n",
    "**Parameter Selection Logic / åƒæ•¸é¸æ“‡é‚è¼¯:**\n",
    "- **eps=30**: Based on distance distribution analysis / åŸºæ–¼è·é›¢åˆ†å¸ƒåˆ†æ\n",
    "  - Same family distances typically < 50 / åŒå®¶æ—è·é›¢é€šå¸¸ < 50\n",
    "  - Different family distances typically > 100 / ä¸åŒå®¶æ—è·é›¢é€šå¸¸ > 100\n",
    "- **min_samples=2**: Minimum cluster size for meaningful groups / æœ‰æ„ç¾©ç¾¤çµ„çš„æœ€å°ç¾¤é›†å¤§å°\n",
    "\n",
    "**Output Structure / è¼¸å‡ºçµæ§‹:**\n",
    "- **Cluster IDs**: [0, 1, 2, ..., 82, -1] where -1 = noise / ç¾¤é›†IDï¼šå…¶ä¸­-1=é›œè¨Š\n",
    "- **Cluster Mapping**: Each sample gets assigned to one cluster or noise / ç¾¤é›†æ˜ å°„ï¼šæ¯å€‹æ¨£æœ¬è¢«åˆ†é…åˆ°ä¸€å€‹ç¾¤é›†æˆ–é›œè¨Š\n",
    "\n",
    "Now let's use the repository's DBSCAN implementation  \n",
    "ç¾åœ¨è®“æˆ‘å€‘ä½¿ç”¨ repository çš„ DBSCAN å¯¦ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run DBSCAN using the repository's implementation\n# ä½¿ç”¨ repository çš„å¯¦ä½œåŸ·è¡Œ DBSCAN\nprint(\"ğŸ”„ Running DBSCAN Clustering / åŸ·è¡Œ DBSCAN åˆ†ç¾¤\")\nprint(\"=\" * 60)\n\n# DBSCAN parameters (same as in repository)\neps = 30  # Distance threshold / è·é›¢é–¾å€¼\nmin_samples = 2  # Minimum samples per cluster / æ¯å€‹ç¾¤é›†çš„æœ€å°æ¨£æœ¬æ•¸\n\nprint(f\"ğŸ“Š DBSCAN Parameters / DBSCAN åƒæ•¸:\")\nprint(f\"   eps (distance threshold): {eps}\")\nprint(f\"   min_samples: {min_samples}\")\nprint(f\"   eps (è·é›¢é–¾å€¼): {eps}\")\nprint(f\"   æœ€å°æ¨£æœ¬æ•¸: {min_samples}\")\n\n# Run DBSCAN using the repository's function\nprint(\"\\nğŸ”„ Processing...\")\n\n# Try to use repository's DBSCAN function with fallback\ntry:\n    # Use repository's runDBSCAN function\n    dbscan_result = runDBSCAN(tlist, eps=eps, min_samples=min_samples)\n    cluster_labels = dbscan_result.labels_\n    print(\"âœ… Used repository's DBSCAN implementation\")\nexcept (NameError, AttributeError) as e:\n    print(f\"âš ï¸ Repository DBSCAN not available: {e}\")\n    print(\"âš ï¸ Using sklearn implementation with TLSH distance matrix\")\n    \n    # Fallback to sklearn implementation\n    from sklearn.cluster import DBSCAN\n    import numpy as np\n    \n    # Create distance matrix using TLSH diff method\n    print(\"ğŸ”„ Computing TLSH distance matrix...\")\n    n_samples = len(tlist)\n    distance_matrix = np.zeros((n_samples, n_samples))\n    \n    # Helper function to calculate TLSH distance\n    def calculate_tlsh_distance(hash1, hash2):\n        try:\n            # Use tlsh.diff() function directly with hash strings\n            return tlsh.diff(hash1, hash2)\n        except Exception as e:\n            # If TLSH calculation fails, use a simple string-based distance\n            return len(set(hash1) - set(hash2)) + len(set(hash2) - set(hash1))\n    \n    for i in range(n_samples):\n        for j in range(i+1, n_samples):\n            dist = calculate_tlsh_distance(tlist[i], tlist[j])\n            distance_matrix[i, j] = dist\n            distance_matrix[j, i] = dist\n        \n        # Progress indicator\n        if i % 100 == 0:\n            print(f\"  Progress: {i}/{n_samples} samples processed\")\n    \n    # Apply DBSCAN\n    print(\"ğŸ”„ Running DBSCAN on distance matrix...\")\n    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n    cluster_labels = dbscan.fit_predict(distance_matrix)\n    \n    print(\"âœ… Used sklearn DBSCAN implementation\")\n\nprint(f\"âœ… DBSCAN clustering completed\")\nprint(f\"âœ… DBSCAN åˆ†ç¾¤å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Clustering Results / æ­¥é©Ÿ 5ï¼šåˆ†æåˆ†ç¾¤çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clustering results / åˆ†æåˆ†ç¾¤çµæœ\n",
    "def analyze_dbscan_results(cluster_labels, family_labels):\n",
    "    \"\"\"Analyze DBSCAN clustering results / åˆ†æ DBSCAN åˆ†ç¾¤çµæœ\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“Š DBSCAN Results Analysis / DBSCAN çµæœåˆ†æ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    unique_clusters = set(cluster_labels)\n",
    "    n_clusters = len(unique_clusters) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Clustering Statistics / åˆ†ç¾¤çµ±è¨ˆ:\")\n",
    "    print(f\"   Number of clusters formed: {n_clusters}\")\n",
    "    print(f\"   Number of noise points: {n_noise}\")\n",
    "    print(f\"   å½¢æˆçš„ç¾¤é›†æ•¸: {n_clusters}\")\n",
    "    print(f\"   é›œè¨Šé»æ•¸: {n_noise}\")\n",
    "    \n",
    "    # Cluster composition analysis\n",
    "    cluster_families = {}\n",
    "    for cluster_id in unique_clusters:\n",
    "        if cluster_id == -1:  # Skip noise\n",
    "            continue\n",
    "        \n",
    "        # Find samples in this cluster\n",
    "        cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]\n",
    "        cluster_family_list = [family_labels[i] for i in cluster_indices]\n",
    "        \n",
    "        # Count families in this cluster\n",
    "        family_counts = Counter(cluster_family_list)\n",
    "        cluster_families[cluster_id] = {\n",
    "            'size': len(cluster_indices),\n",
    "            'families': family_counts,\n",
    "            'dominant_family': family_counts.most_common(1)[0]\n",
    "        }\n",
    "    \n",
    "    return cluster_families, n_clusters, n_noise\n",
    "\n",
    "cluster_families, n_clusters, n_noise = analyze_dbscan_results(cluster_labels, family_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top clusters / é¡¯ç¤ºä¸»è¦ç¾¤é›†\n",
    "print(\"\\nğŸ† Top 10 Largest Clusters / å‰10å¤§ç¾¤é›†:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sort clusters by size\n",
    "sorted_clusters = sorted(cluster_families.items(), key=lambda x: x[1]['size'], reverse=True)\n",
    "\n",
    "for i, (cluster_id, info) in enumerate(sorted_clusters[:10]):\n",
    "    dominant_family, dominant_count = info['dominant_family']\n",
    "    purity = (dominant_count / info['size']) * 100\n",
    "    \n",
    "    print(f\"Cluster {cluster_id:2d}: {info['size']:3d} samples, {dominant_family:15} ({purity:5.1f}% purity)\")\n",
    "    \n",
    "    # Show family distribution if mixed\n",
    "    if len(info['families']) > 1:\n",
    "        family_str = \", \".join([f\"{family}({count})\" for family, count in info['families'].most_common()])\n",
    "        print(f\"          Mixed: {family_str}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Clustering Quality Insights / ç¾¤é›†å“è³ªæ´å¯Ÿ:\")\n",
    "print(f\"   ğŸ“Š Pure clusters (100% same family) = Excellent clustering / ç´”ç¾¤é›†ï¼ˆ100%åŒå®¶æ—ï¼‰= å„ªç§€åˆ†ç¾¤\")\n",
    "print(f\"   ğŸ“Š Mixed clusters = Potential data relationships / æ··åˆç¾¤é›† = æ½›åœ¨è³‡æ–™é—œä¿‚\") \n",
    "print(f\"   ğŸ“Š Many small clusters = High data diversity / è¨±å¤šå°ç¾¤é›† = é«˜è³‡æ–™å¤šæ¨£æ€§\")\n",
    "print(f\"   ğŸ“Š High noise ratio = Need parameter tuning / é«˜é›œè¨Šæ¯”ç‡ = éœ€è¦åƒæ•¸èª¿æ•´\")\n",
    "\n",
    "print(f\"\\nğŸ’¼ Enterprise Data Lineage Interpretation / ä¼æ¥­è³‡æ–™è­œç³»è§£é‡‹:\")\n",
    "print(f\"   ğŸ” Large pure clusters â†’ Same dataset copied multiple times / å¤§å‹ç´”ç¾¤é›† â†’ åŒä¸€è³‡æ–™é›†è¢«å¤šæ¬¡è¤‡è£½\")\n",
    "print(f\"   ğŸ” Mixed clusters â†’ Related datasets with modifications / æ··åˆç¾¤é›† â†’ æœ‰ä¿®æ”¹çš„ç›¸é—œè³‡æ–™é›†\")\n",
    "print(f\"   ğŸ” Fragmented families â†’ Data variants or different formats / åˆ†æ•£å®¶æ— â†’ è³‡æ–™è®Šç¨®æˆ–ä¸åŒæ ¼å¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Family-based Analysis / æ­¥é©Ÿ 6ï¼šåŸºæ–¼å®¶æ—çš„åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how well families are clustered / åˆ†æå®¶æ—åˆ†ç¾¤æ•ˆæœ\n",
    "def analyze_family_clustering(cluster_labels, family_labels):\n",
    "    \"\"\"Analyze how malware families are distributed across clusters\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¬ Family Clustering Analysis / å®¶æ—åˆ†ç¾¤åˆ†æ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get top families\n",
    "    family_counts = Counter(family_labels)\n",
    "    top_families = family_counts.most_common(5)\n",
    "    \n",
    "    for family, total_count in top_families:\n",
    "        print(f\"\\nğŸ“ Family: {family} ({total_count} samples)\")\n",
    "        \n",
    "        # Find where this family's samples ended up\n",
    "        family_indices = [i for i, f in enumerate(family_labels) if f == family]\n",
    "        family_clusters = [cluster_labels[i] for i in family_indices]\n",
    "        \n",
    "        cluster_distribution = Counter(family_clusters)\n",
    "        \n",
    "        for cluster_id, count in cluster_distribution.most_common():\n",
    "            if cluster_id == -1:\n",
    "                print(f\"   Noise: {count} samples\")\n",
    "                print(f\"   é›œè¨Š: {count} å€‹æ¨£æœ¬\")\n",
    "            else:\n",
    "                percentage = (count / total_count) * 100\n",
    "                print(f\"   Cluster {cluster_id}: {count} samples ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Calculate fragmentation\n",
    "        n_clusters_for_family = len([c for c in cluster_distribution.keys() if c != -1])\n",
    "        if n_clusters_for_family > 1:\n",
    "            print(f\"   âš ï¸  Family is fragmented across {n_clusters_for_family} clusters\")\n",
    "            print(f\"   âš ï¸  å®¶æ—åˆ†æ•£åœ¨ {n_clusters_for_family} å€‹ç¾¤é›†ä¸­\")\n",
    "        else:\n",
    "            print(f\"   âœ… Family is well-clustered\")\n",
    "            print(f\"   âœ… å®¶æ—åˆ†ç¾¤è‰¯å¥½\")\n",
    "\n",
    "analyze_family_clustering(cluster_labels, family_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Results / æ­¥é©Ÿ 7ï¼šçµæœè¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization / å»ºç«‹ç¶œåˆè¦–è¦ºåŒ–\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Cluster size distribution\n",
    "cluster_sizes = [info['size'] for info in cluster_families.values()]\n",
    "ax1.hist(cluster_sizes, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Cluster Size / ç¾¤é›†å¤§å°')\n",
    "ax1.set_ylabel('Frequency / é »ç‡')\n",
    "ax1.set_title('Cluster Size Distribution / ç¾¤é›†å¤§å°åˆ†å¸ƒ')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Top families distribution\n",
    "family_counts = Counter(family_labels)\n",
    "top_5_families = family_counts.most_common(5)\n",
    "families, counts = zip(*top_5_families)\n",
    "\n",
    "ax2.bar(range(len(families)), counts, color='lightgreen', alpha=0.7)\n",
    "ax2.set_xlabel('Malware Families / æƒ¡æ„è»Ÿé«”å®¶æ—')\n",
    "ax2.set_ylabel('Sample Count / æ¨£æœ¬æ•¸é‡')\n",
    "ax2.set_title('Top 5 Malware Families / å‰5å¤§æƒ¡æ„è»Ÿé«”å®¶æ—')\n",
    "ax2.set_xticks(range(len(families)))\n",
    "ax2.set_xticklabels(families, rotation=45, ha='right')\n",
    "\n",
    "# 3. Clustering effectiveness\n",
    "n_total_samples = len(cluster_labels)\n",
    "n_clustered = n_total_samples - n_noise\n",
    "\n",
    "labels_pie = ['Clustered\\nåˆ†ç¾¤', 'Noise\\né›œè¨Š']\n",
    "sizes_pie = [n_clustered, n_noise]\n",
    "colors_pie = ['lightblue', 'lightcoral']\n",
    "\n",
    "ax3.pie(sizes_pie, labels=labels_pie, colors=colors_pie, autopct='%1.1f%%', startangle=90)\n",
    "ax3.set_title('Clustering Effectiveness / åˆ†ç¾¤æœ‰æ•ˆæ€§')\n",
    "\n",
    "# 4. Cluster purity analysis\n",
    "purities = []\n",
    "for info in cluster_families.values():\n",
    "    dominant_count = info['dominant_family'][1]\n",
    "    purity = (dominant_count / info['size']) * 100\n",
    "    purities.append(purity)\n",
    "\n",
    "ax4.hist(purities, bins=10, alpha=0.7, color='gold', edgecolor='black')\n",
    "ax4.set_xlabel('Cluster Purity (%) / ç¾¤é›†ç´”åº¦ (%)')\n",
    "ax4.set_ylabel('Number of Clusters / ç¾¤é›†æ•¸é‡')\n",
    "ax4.set_title('Cluster Purity Distribution / ç¾¤é›†ç´”åº¦åˆ†å¸ƒ')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nğŸ“Š Final Summary / æœ€çµ‚æ‘˜è¦:\")\n",
    "print(f\"âœ… Processed {len(tlist)} malware samples from {len(set(family_labels))} families\")\n",
    "print(f\"âœ… è™•ç†äº†ä¾†è‡ª {len(set(family_labels))} å€‹å®¶æ—çš„ {len(tlist)} å€‹æƒ¡æ„è»Ÿé«”æ¨£æœ¬\")\n",
    "print(f\"âœ… DBSCAN found {n_clusters} clusters with {n_noise} noise points\")\n",
    "print(f\"âœ… DBSCAN æ‰¾åˆ° {n_clusters} å€‹ç¾¤é›†ï¼Œ{n_noise} å€‹é›œè¨Šé»\")\n",
    "print(f\"âœ… Average cluster purity: {np.mean(purities):.1f}%\")\n",
    "print(f\"âœ… å¹³å‡ç¾¤é›†ç´”åº¦: {np.mean(purities):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Results (Optional) / æ­¥é©Ÿ 8ï¼šä¿å­˜çµæœï¼ˆå¯é¸ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustering results to file (similar to repository output format)\n",
    "# å°‡åˆ†ç¾¤çµæœä¿å­˜åˆ°æª”æ¡ˆï¼ˆé¡ä¼¼ repository è¼¸å‡ºæ ¼å¼ï¼‰\n",
    "output_filename = \"workshop_dbscan_results.txt\"\n",
    "\n",
    "print(f\"ğŸ’¾ Saving results to {output_filename}\")\n",
    "print(f\"ğŸ’¾ å°‡çµæœä¿å­˜åˆ° {output_filename}\")\n",
    "\n",
    "try:\n",
    "    # Try to use repository's outputClusters function\n",
    "    outputClusters(output_filename, tlist, cluster_labels, labels, quiet=True)\n",
    "    print(f\"âœ… Used repository's output function\")\n",
    "except (NameError, AttributeError) as e:\n",
    "    print(\"âš ï¸ Repository output function not available, creating simple output\")\n",
    "    # Simple fallback output\n",
    "    with open(output_filename, 'w') as f:\n",
    "        f.write(\"# TLSH Clustering Results\\n\")\n",
    "        f.write(f\"# Total samples: {len(tlist)}\\n\")\n",
    "        f.write(f\"# DBSCAN parameters: eps={30}, min_samples={2}\\n\")\n",
    "        f.write(f\"# Clusters found: {len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)}\\n\")\n",
    "        f.write(f\"# Noise points: {list(cluster_labels).count(-1)}\\n\")\n",
    "        f.write(\"#\\n\")\n",
    "        f.write(\"# Format: TLSH_Hash,Family_Label,Cluster_ID\\n\")\n",
    "        \n",
    "        for i, (tlsh_hash, cluster_id) in enumerate(zip(tlist, cluster_labels)):\n",
    "            family_label = family_labels[i] if i < len(family_labels) else \"Unknown\"\n",
    "            f.write(f\"{tlsh_hash},{family_label},{cluster_id}\\n\")\n",
    "\n",
    "print(f\"âœ… Results saved successfully\")\n",
    "print(f\"âœ… çµæœä¿å­˜æˆåŠŸ\")\n",
    "\n",
    "# Show first few lines of output\n",
    "print(f\"\\nğŸ“„ Preview of output file:\")\n",
    "try:\n",
    "    with open(output_filename, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 10:  # Show first 10 lines\n",
    "                break\n",
    "            print(line.strip())\n",
    "except FileNotFoundError:\n",
    "    print(\"Output file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Workshop Summary / å·¥ä½œåŠç¸½çµ\n",
    "\n",
    "### What We Accomplished / æˆ‘å€‘å®Œæˆäº†ä»€éº¼ï¼š\n",
    "\n",
    "1. **âœ… Loaded real malware dataset** (999 samples, 52 families)  \n",
    "   **âœ… è¼‰å…¥çœŸå¯¦æƒ¡æ„è»Ÿé«”è³‡æ–™é›†**ï¼ˆ999å€‹æ¨£æœ¬ï¼Œ52å€‹å®¶æ—ï¼‰\n",
    "\n",
    "2. **âœ… Analyzed TLSH distance patterns** between malware families  \n",
    "   **âœ… åˆ†ææƒ¡æ„è»Ÿé«”å®¶æ—é–“çš„ TLSH è·é›¢æ¨¡å¼**\n",
    "\n",
    "3. **âœ… Applied DBSCAN clustering** using repository's production code  \n",
    "   **âœ… ä½¿ç”¨ repository çš„ç”Ÿç”¢ç¨‹å¼ç¢¼æ‡‰ç”¨ DBSCAN åˆ†ç¾¤**\n",
    "\n",
    "4. **âœ… Evaluated clustering quality** through purity and family analysis  \n",
    "   **âœ… é€šéç´”åº¦å’Œå®¶æ—åˆ†æè©•ä¼°åˆ†ç¾¤å“è³ª**\n",
    "\n",
    "### Key Insights / ä¸»è¦æ´å¯Ÿï¼š\n",
    "\n",
    "- **TLSH effectively groups similar malware** within families  \n",
    "  **TLSH æœ‰æ•ˆåœ°å°‡ç›¸ä¼¼æƒ¡æ„è»Ÿé«”æ­¸çµ„åˆ°åŒä¸€å®¶æ—**\n",
    "\n",
    "- **DBSCAN parameters (eps=30, min_samples=2)** work well for malware clustering  \n",
    "  **DBSCAN åƒæ•¸ï¼ˆeps=30, min_samples=2ï¼‰é©åˆæƒ¡æ„è»Ÿé«”åˆ†ç¾¤**\n",
    "\n",
    "- **Some families fragment** across multiple clusters, indicating variants  \n",
    "  **æŸäº›å®¶æ—åˆ†æ•£åˆ°å¤šå€‹ç¾¤é›†ï¼Œè¡¨ç¤ºå­˜åœ¨è®Šç¨®**\n",
    "\n",
    "- **Mixed clusters** may reveal related malware families  \n",
    "  **æ··åˆç¾¤é›†å¯èƒ½æ­ç¤ºç›¸é—œçš„æƒ¡æ„è»Ÿé«”å®¶æ—**\n",
    "\n",
    "### ğŸ¢ Real-World Applications / çœŸå¯¦ä¸–ç•Œæ‡‰ç”¨ï¼š\n",
    "\n",
    "**1. Financial Services Data Governance / é‡‘èæœå‹™è³‡æ–™æ²»ç†:**\n",
    "- **Use Case**: Bank needs to track customer transaction logs across 50+ systems  \n",
    "  ä½¿ç”¨æ¡ˆä¾‹ï¼šéŠ€è¡Œéœ€è¦è·¨è¶Š50å¤šå€‹ç³»çµ±è¿½è¹¤å®¢æˆ¶äº¤æ˜“è¨˜éŒ„\n",
    "- **TLSH Solution**: Identify duplicate/leaked customer data in staging/test environments  \n",
    "  TLSHè§£æ±ºæ–¹æ¡ˆï¼šè­˜åˆ¥æ¸¬è©¦ç’°å¢ƒä¸­çš„é‡è¤‡/æ´©æ¼å®¢æˆ¶è³‡æ–™\n",
    "- **Business Impact**: Prevent regulatory violations, ensure PII compliance  \n",
    "  å•†æ¥­å½±éŸ¿ï¼šé é˜²æ³•è¦é•è¦ï¼Œç¢ºä¿å€‹è³‡åˆè¦\n",
    "\n",
    "**2. Healthcare Data Lineage / é†«ç™‚è³‡æ–™è­œç³»:**\n",
    "- **Use Case**: Hospital system with patient data across multiple departments  \n",
    "  ä½¿ç”¨æ¡ˆä¾‹ï¼šé†«é™¢ç³»çµ±çš„ç—…æ‚£è³‡æ–™åˆ†æ•£åœ¨å¤šå€‹éƒ¨é–€\n",
    "- **TLSH Solution**: Track patient records and ensure data consistency  \n",
    "  TLSHè§£æ±ºæ–¹æ¡ˆï¼šè¿½è¹¤ç—…æ‚£è¨˜éŒ„ä¸¦ç¢ºä¿è³‡æ–™ä¸€è‡´æ€§\n",
    "- **Business Impact**: HIPAA compliance, medical audit trails  \n",
    "  å•†æ¥­å½±éŸ¿ï¼šHIPAAåˆè¦ï¼Œé†«ç™‚ç¨½æ ¸è»Œè·¡\n",
    "\n",
    "**3. E-commerce Platform Security / é›»å•†å¹³å°å®‰å…¨:**\n",
    "- **Use Case**: Multi-vendor platform needs to monitor data sharing between partners  \n",
    "  ä½¿ç”¨æ¡ˆä¾‹ï¼šå¤šä¾›æ‡‰å•†å¹³å°éœ€è¦ç›£æ§åˆä½œå¤¥ä¼´é–“çš„è³‡æ–™å…±äº«\n",
    "- **TLSH Solution**: Detect unauthorized data copying or modification  \n",
    "  TLSHè§£æ±ºæ–¹æ¡ˆï¼šæª¢æ¸¬æœªæˆæ¬Šçš„è³‡æ–™è¤‡è£½æˆ–ä¿®æ”¹\n",
    "- **Business Impact**: Supply chain security, vendor compliance monitoring  \n",
    "  å•†æ¥­å½±éŸ¿ï¼šä¾›æ‡‰éˆå®‰å…¨ï¼Œä¾›æ‡‰å•†åˆè¦ç›£æ§\n",
    "\n",
    "### Next Steps / ä¸‹ä¸€æ­¥ï¼š\n",
    "\n",
    "- Try different eps values (20, 40, 50) to see clustering changes  \n",
    "  å˜—è©¦ä¸åŒçš„ eps å€¼ï¼ˆ20, 40, 50ï¼‰è§€å¯Ÿåˆ†ç¾¤è®ŠåŒ–\n",
    "- Implement automated data lineage dashboard with real-time TLSH monitoring  \n",
    "  å¯¦ä½œè‡ªå‹•åŒ–è³‡æ–™è­œç³»å„€è¡¨æ¿ä¸¦å³æ™‚ç›£æ§TLSH\n",
    "- Integrate with enterprise data catalogs for comprehensive governance  \n",
    "  èˆ‡ä¼æ¥­è³‡æ–™ç›®éŒ„æ•´åˆä»¥å¯¦ç¾å…¨é¢æ²»ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}